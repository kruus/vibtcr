{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5134b8c9",
   "metadata": {},
   "source": [
    "# Out-Of-Distribution Detection (OOD) for TCR recognition \n",
    "\n",
    "In this notebook, we investigate the effect of the beta hyperapram. on OOD detection.\n",
    "The in-distribution dataset is the `α+β set`+`β set` (i.e. the human data); the out-of-distribtuion dataset is the `non-human set` (mouse + macaque).\n",
    "\n",
    "The goal of this study is to come up with a novel OOD detection method and benchmark it to a set of baseline methods.\n",
    "Goal: only the human `α+β set`+`β set` (in-distribution dataset) is available at training time. We have no access to the `non-human set` (out-of-distribution dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e239f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from vibtcr.dataset import TCRDataset\n",
    "from vibtcr.mvib.mvib import MVIB\n",
    "from vibtcr.mvib.mvib_trainer import TrainerMVIB\n",
    "\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac089b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "metrics = [\n",
    "    'auROC',\n",
    "    'Accuracy',\n",
    "    'Recall',\n",
    "    'Precision',\n",
    "    'F1 score',\n",
    "    'auPRC'\n",
    "]\n",
    "\n",
    "def pr_auc(y_true, y_prob):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    return pr_auc\n",
    "\n",
    "def get_scores(y_true, y_prob, y_pred):\n",
    "    \"\"\"\n",
    "    Compute a df with all classification metrics and respective scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = [\n",
    "        roc_auc_score(y_true, y_prob),\n",
    "        accuracy_score(y_true, y_pred),\n",
    "        recall_score(y_true, y_pred),\n",
    "        precision_score(y_true, y_pred),\n",
    "        f1_score(y_true, y_pred),\n",
    "        pr_auc(y_true, y_prob)\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(data={'score': scores, 'metrics': metrics})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "515d629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7227512",
   "metadata": {},
   "outputs": [],
   "source": [
    "login = os.getlogin( )\n",
    "\n",
    "DATA_BASE = f\"/home/{login}/Git/tcr/data/\"\n",
    "RESULTS_BASE = f\"/home/{login}/Git/tcr/notebooks/notebooks.ood/results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "599fe993",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2')\n",
    "test_device = torch.device('cuda:4')\n",
    "\n",
    "batch_size = 4096\n",
    "epochs = 200\n",
    "lr = 1e-3\n",
    "\n",
    "z_dim = 150\n",
    "early_stopper_patience = 20\n",
    "monitor = 'auROC'\n",
    "lr_scheduler_param = 10\n",
    "joint_posterior = \"aoe\"\n",
    "\n",
    "beta = 1e-4 # 0, 1e-6, 1e-4, 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21313546",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a3e5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = pd.concat([\n",
    "    pd.read_csv(DATA_BASE + 'alpha-beta-splits/alpha-beta.csv'),\n",
    "    pd.read_csv(DATA_BASE + 'alpha-beta-splits/beta.csv')\n",
    "    ])\n",
    "\n",
    "df_out = pd.read_csv(DATA_BASE + 'vdjdb/vdjdb-2021-09-05/mouse-macaco.csv')\n",
    "\n",
    "if os.path.isfile(RESULTS_BASE+f'avib.beta-{beta}.rep-0.pth.tar'):\n",
    "    checkpoints = [\n",
    "        torch.load(RESULTS_BASE+f'avib.beta-{beta}.rep-{i}.pth.tar')\n",
    "        for i in range(5)\n",
    "    ]\n",
    "else:\n",
    "    checkpoints = []\n",
    "\n",
    "if len(checkpoints) == 0:\n",
    "    for i in range(5):  # 5 independent train/test splits\n",
    "        set_random_seed(i)\n",
    "\n",
    "        df_train, df_test_in = train_test_split(df_in.copy(), test_size=0.2, random_state=i)\n",
    "        df_test_out = df_out\n",
    "        df_test_in = df_test_in.sample(n=len(df_test_out))\n",
    "        scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, softmax=True).scaler\n",
    "\n",
    "        df_test_in['sign'] = 1  #in-distribution test set\n",
    "        df_test_out['sign'] = 0  #out-of-distribution test set\n",
    "        df_test = pd.concat([df_test_in, df_test_out])\n",
    "        ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler, softmax=True)\n",
    "\n",
    "        df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "\n",
    "        # train loader with balanced sampling\n",
    "        ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler, softmax=True)\n",
    "        class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "        weight = 1. / class_count\n",
    "        samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            ds_train,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler\n",
    "        )\n",
    "\n",
    "        # val loader with balanced sampling\n",
    "        ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, softmax=True, scaler=scaler)\n",
    "        class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "        weight = 1. / class_count\n",
    "        samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            ds_val,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler\n",
    "        )\n",
    "\n",
    "        model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior, softmax=True).to(device)\n",
    "\n",
    "        trainer = TrainerMVIB(\n",
    "            model,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            beta=beta,\n",
    "            checkpoint_dir=\".\",\n",
    "            mode=\"bimodal\",\n",
    "            lr_scheduler_param=lr_scheduler_param\n",
    "        )\n",
    "        checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)\n",
    "        trainer.save_checkpoint(checkpoint, folder='./', filename=RESULTS_BASE+f'avib.beta-{beta}.rep-{i}.pth.tar')\n",
    "        checkpoints.append(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3541f0",
   "metadata": {},
   "source": [
    "# Mahalanobis distance\n",
    "Lee et al., NIPS 2018\n",
    "\n",
    "https://arxiv.org/abs/1807.03888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "637d1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from https://github.com/pokaxpoka/deep_Mahalanobis_detector\n",
    "\n",
    "import sklearn.covariance\n",
    "from torch.autograd import Variable\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "\n",
    "def get_Mahalanobis_score(model, test_loader, num_classes, sample_mean, precision, layer_index, magnitude, mean, scale, cdr3b_only = False):\n",
    "    '''\n",
    "    Compute the proposed Mahalanobis confidence score on input dataset\n",
    "    return: Mahalanobis score from layer_index\n",
    "    '''\n",
    "    model.eval()\n",
    "    Mahalanobis = []\n",
    "    \n",
    "    for pep, cdr3b, target in test_loader:\n",
    "        pep, cdr3b, target = Variable(pep, requires_grad = True), Variable(cdr3b, requires_grad = True), Variable(target)\n",
    "        \n",
    "        if cdr3b_only:\n",
    "            if layer_index == 0:\n",
    "                out_features = model.encoder_cdr3b(cdr3b)[0]  # mu\n",
    "            else:\n",
    "                raise NotImplementedError(\"if cdr3b_only, layer_index must be 0\")\n",
    "        else:\n",
    "            if layer_index == 0:\n",
    "                out_features = model(pep, cdr3b)[0]  # mu\n",
    "            else:\n",
    "                out_features = model(pep, cdr3b)[2]  # classification logits\n",
    "\n",
    "        # compute Mahalanobis score\n",
    "        gaussian_score = 0\n",
    "        for i in range(num_classes):\n",
    "            batch_sample_mean = sample_mean[layer_index][i]\n",
    "            zero_f = out_features.data - batch_sample_mean\n",
    "            p = precision[layer_index]\n",
    "            term_gau = -0.5*torch.mm(torch.mm(zero_f, p), zero_f.t()).diag()\n",
    "            if i == 0:\n",
    "                gaussian_score = term_gau.view(-1,1)\n",
    "            else:\n",
    "                gaussian_score = torch.cat((gaussian_score, term_gau.view(-1,1)), 1)\n",
    "\n",
    "        # Input_processing\n",
    "        sample_pred = gaussian_score.max(1)[1]\n",
    "        batch_sample_mean = sample_mean[layer_index].index_select(0, sample_pred)\n",
    "        zero_f = out_features - Variable(batch_sample_mean)\n",
    "        p = Variable(precision[layer_index])\n",
    "        pure_gau = -0.5*torch.mm(torch.mm(zero_f, p), zero_f.t()).diag()\n",
    "        loss = torch.mean(-pure_gau)\n",
    "        loss.backward()\n",
    "        \n",
    "        if not cdr3b_only:\n",
    "            gradient_pep =  torch.ge(pep.grad.data, 0)\n",
    "            gradient_pep = (gradient_pep.float() - 0.5) * 2\n",
    "            # normalizing the gradient to the same space of input\n",
    "            gradient_pep = (gradient_pep.transpose(1,2) / torch.tensor(mean/scale).float()).transpose(1,2)\n",
    "            tempInputs_pep = torch.add(pep.data, -magnitude, gradient_pep)\n",
    "          \n",
    "        gradient_cdr3b =  torch.ge(cdr3b.grad.data, 0)\n",
    "        gradient_cdr3b = (gradient_cdr3b.float() - 0.5) * 2\n",
    "        # normalizing the gradient to the same space of input\n",
    "        gradient_cdr3b = (gradient_cdr3b.transpose(1,2) / torch.tensor(mean/scale).float()).transpose(1,2)\n",
    "        tempInputs_cdr3b = torch.add(cdr3b.data, -magnitude, gradient_cdr3b)\n",
    "\n",
    "        if cdr3b_only:\n",
    "            if layer_index == 0:\n",
    "                noise_out_features = model.encoder_cdr3b(Variable(tempInputs_cdr3b))[0]  # mu\n",
    "            else:\n",
    "                raise NotImplementedError(\"if cdr3b_only, layer_index must be 0\")\n",
    "        else:\n",
    "            if layer_index == 0:\n",
    "                noise_out_features = model(\n",
    "                    Variable(tempInputs_pep), Variable(tempInputs_cdr3b)\n",
    "                )[0]  # mu\n",
    "            else:\n",
    "                noise_out_features = model(\n",
    "                    Variable(tempInputs_pep), Variable(tempInputs_cdr3b)\n",
    "                )[2]  # classification logits\n",
    "\n",
    "        with torch.no_grad():\n",
    "            noise_out_features = noise_out_features.view(noise_out_features.size(0), noise_out_features.size(1), -1)\n",
    "            noise_out_features = torch.mean(noise_out_features, 2)\n",
    "            noise_gaussian_score = 0\n",
    "            for i in range(num_classes):\n",
    "                batch_sample_mean = sample_mean[layer_index][i]\n",
    "                zero_f = noise_out_features.data - batch_sample_mean\n",
    "                term_gau = -0.5*torch.mm(torch.mm(zero_f, precision[layer_index]), zero_f.t()).diag()\n",
    "                if i == 0:\n",
    "                    noise_gaussian_score = term_gau.view(-1,1)\n",
    "                else:\n",
    "                    noise_gaussian_score = torch.cat((noise_gaussian_score, term_gau.view(-1,1)), 1)      \n",
    "\n",
    "        noise_gaussian_score, _ = torch.max(noise_gaussian_score, dim=1)\n",
    "        Mahalanobis.extend(noise_gaussian_score.detach().cpu().numpy())\n",
    "\n",
    "    return Mahalanobis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = pd.concat([\n",
    "    pd.read_csv(DATA_BASE + 'alpha-beta-splits/alpha-beta.csv'),\n",
    "    pd.read_csv(DATA_BASE + 'alpha-beta-splits/beta.csv')\n",
    "    ])\n",
    "\n",
    "df_out = pd.read_csv(DATA_BASE + 'vdjdb/vdjdb-2021-09-05/mouse-macaco.csv')\n",
    "\n",
    "for experiment in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(experiment)\n",
    "\n",
    "    df_train, df_test_in = train_test_split(df_in.copy(), test_size=0.2, random_state=experiment)\n",
    "    df_test_out = df_out\n",
    "    df_test_in = df_test_in.sample(n=len(df_test_out))\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, softmax=True).scaler\n",
    "    \n",
    "    df_test_in['sign'] = 1  #in-distribution test set\n",
    "    df_test_out['sign'] = 0  #out-of-distribution test set\n",
    "    df_test = pd.concat([df_test_in, df_test_out])\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler, softmax=True)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=experiment)\n",
    "\n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler, softmax=True)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, softmax=True, scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    # test loader for Mahalanobis distance\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        ds_test,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    checkpoint = checkpoints[experiment]       \n",
    "\n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    model.eval()\n",
    "            \n",
    "    for layer_index in [0]:  # 0 = mu - means of latent Gaussian; 1 = pre-softmax logits\n",
    "        for magnitude in [0]:  # different magnitude for gradient pre-processing\n",
    "            ############# Peptide + CDR3b joint posterior #############\n",
    "            sample_mean = []\n",
    "            precision = []\n",
    "\n",
    "            # get training samples of class 0\n",
    "            pep_0 = ds_train.pep[ds_train.gt.argmax(dim=1) == torch.tensor(0)].cpu()\n",
    "            cdr3b_0 = ds_train.cdr3b[ds_train.gt.argmax(dim=1) == torch.tensor(0)].cpu()\n",
    "\n",
    "            # get training samples of class 1\n",
    "            pep_1 = ds_train.pep[ds_train.gt.argmax(dim=1) == torch.tensor(1)].cpu()\n",
    "            cdr3b_1 = ds_train.cdr3b[ds_train.gt.argmax(dim=1) == torch.tensor(1)].cpu()\n",
    "\n",
    "            # pass the training samples through the trained model\n",
    "            # get latent mu (mean fo the latent joint posterior) for each class\n",
    "            samples_mu = [\n",
    "                model(pep=pep_0, cdr3b=cdr3b_0, cdr3a=None)[0],\n",
    "                model(pep=pep_1, cdr3b=cdr3b_1, cdr3a=None)[0],\n",
    "            ]\n",
    "\n",
    "            # pass the training samples through the trained model\n",
    "            # get latent pre-softmax logits for each class\n",
    "            samples_logits = [\n",
    "                model(pep=pep_0, cdr3b=cdr3b_0, cdr3a=None)[2],\n",
    "                model(pep=pep_1, cdr3b=cdr3b_1, cdr3a=None)[2],\n",
    "            ]\n",
    "\n",
    "            # compute mean of the mu for each class\n",
    "            samples_mean_mu = torch.Tensor(2, z_dim)\n",
    "            samples_mean_mu[0] = model(pep=pep_0, cdr3b=cdr3b_0, cdr3a=None)[0].mean(dim=0)\n",
    "            samples_mean_mu[1] = model(pep=pep_1, cdr3b=cdr3b_1, cdr3a=None)[0].mean(dim=0)\n",
    "\n",
    "            # compute mean of the pre-softmax logits for each class\n",
    "            samples_mean_logits = torch.Tensor(2, 2)\n",
    "            samples_mean_logits[0] = model(pep=pep_0, cdr3b=cdr3b_0, cdr3a=None)[2].mean(dim=0)\n",
    "            samples_mean_logits[1] = model(pep=pep_1, cdr3b=cdr3b_1, cdr3a=None)[2].mean(dim=0)\n",
    "\n",
    "            sample_mean.append(samples_mean_mu)\n",
    "            sample_mean.append(samples_mean_logits)\n",
    "\n",
    "            group_lasso = sklearn.covariance.EmpiricalCovariance(assume_centered=False)\n",
    "\n",
    "            # construct inverse of covariance matrix for mu\n",
    "            X = 0\n",
    "            for i in range(2):\n",
    "                if i == 0:\n",
    "                    X = samples_mu[i] - sample_mean[0][i]\n",
    "                else:\n",
    "                    X = torch.cat((X, samples_mu[i] - sample_mean[0][i]), 0)\n",
    "\n",
    "            # find inverse            \n",
    "            group_lasso.fit(X.detach().cpu().numpy())\n",
    "            temp_precision = group_lasso.precision_\n",
    "            temp_precision = torch.from_numpy(temp_precision).float()\n",
    "            precision.append(temp_precision)\n",
    "\n",
    "            # construct inverse of covariance matrix for pre-softmax logits\n",
    "            X = 0\n",
    "            for i in range(2):\n",
    "                if i == 0:\n",
    "                    X = samples_logits[i] - sample_mean[1][i]\n",
    "                else:\n",
    "                    X = torch.cat((X, samples_logits[i] - sample_mean[1][i]), 0)\n",
    "\n",
    "            # find inverse            \n",
    "            group_lasso.fit(X.detach().cpu().numpy())\n",
    "            temp_precision = group_lasso.precision_\n",
    "            temp_precision = torch.from_numpy(temp_precision).float()\n",
    "            precision.append(temp_precision)\n",
    "\n",
    "            # compute Mahalanobis distances for all test samples (peptide+CDR3b)\n",
    "            maha = get_Mahalanobis_score(model, test_loader, 2, sample_mean, precision, layer_index, magnitude, scaler.mean_, scaler.scale_, cdr3b_only=False)\n",
    "            df_test['prediction_'+str(experiment)] = maha\n",
    "\n",
    "            # save results for further analysis\n",
    "            df_test.to_csv(\n",
    "                RESULTS_BASE + f\"non-human.mvib.{joint_posterior}.beta-{beta}.maha.layer-{layer_index}.epsilon-{magnitude}.rep-{experiment}.csv\",\n",
    "                index=False\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112d7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
