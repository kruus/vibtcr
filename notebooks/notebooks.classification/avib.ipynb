{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808e2dcc",
   "metadata": {},
   "source": [
    "# Attentive Variational Information Bottleneck\n",
    "\n",
    "In this notebook, we train and test the Attentive Variational Information Bottleneck (MVIB [1] with Attention of Experts) and MVIB on all datasets.\n",
    "\n",
    "[1] Microbiome-based disease prediction with multimodal variational information bottlenecks, Grazioli et al., https://www.biorxiv.org/node/2109522.external-links.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fdd9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from vibtcr.dataset import TCRDataset\n",
    "from vibtcr.mvib.mvib import MVIB\n",
    "from vibtcr.mvib.mvib_trainer import TrainerMVIB\n",
    "\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa957077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "metrics = [\n",
    "    'auROC',\n",
    "    'Accuracy',\n",
    "    'Recall',\n",
    "    'Precision',\n",
    "    'F1 score',\n",
    "    'auPRC'\n",
    "]\n",
    "\n",
    "def pr_auc(y_true, y_prob):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    return pr_auc\n",
    "\n",
    "def get_scores(y_true, y_prob, y_pred):\n",
    "    \"\"\"\n",
    "    Compute a df with all classification metrics and respective scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = [\n",
    "        roc_auc_score(y_true, y_prob),\n",
    "        accuracy_score(y_true, y_pred),\n",
    "        recall_score(y_true, y_pred),\n",
    "        precision_score(y_true, y_pred),\n",
    "        f1_score(y_true, y_pred),\n",
    "        pr_auc(y_true, y_prob)\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(data={'score': scores, 'metrics': metrics})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "314146e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133637c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "login = os.getlogin( )\n",
    "DATA_BASE = f\"/home/{login}/Git/tcr/data/\"\n",
    "RESULTS_BASE = f\"/home/{login}/Git/tcr/notebooks/notebooks.classification/results/\"\n",
    "# To run in github checkout of vibtcr, after `unzip data.zip` ...\n",
    "RESULTS_BASE = os.path.join('.', 'results')\n",
    "DATA_BASE = os.path.join('..', '..', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a04b4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "batch_size = 4096\n",
    "epochs = 500 #500 <-------------------------------------\n",
    "lr = 1e-3\n",
    "\n",
    "z_dim = 150\n",
    "early_stopper_patience = 50\n",
    "monitor = 'auROC'\n",
    "lr_scheduler_param = 10\n",
    "\n",
    "beta = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2174113-3ad9-45fb-9173-1245c127060d",
   "metadata": {},
   "source": [
    "# AVIB multimodal pooling of experts (aoe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ae95e2d-4abd-4ec0-b210-178f48e1b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# NOTE: This notebook runs several choices for \"joint_posterior\"\n",
    "#       Class BaseVIB supports:\n",
    "#         'peo' ~ \"product of experts\" ~ \"MVIB\" (multimodal) original paper\n",
    "#         'aoe' ~ \"attention of experts\" ~ \"AVIB\" (attentive)\n",
    "#         'max_pool'\n",
    "#         'avg_pool'\n",
    "joint_posterior = \"aoe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794a7f3",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3b (AVIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77f5b486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 115 | Best val score -0.913730 | DKL-prior 0.000494 | BCE 0.811390 | auROC 0.9137:  33%|████████████████████████████████████████████████████████▍                                                                                                                   | 164/500 [07:37<15:38,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 49 | Best val score -0.915488 | DKL-prior 0.000439 | BCE 0.505180 | auROC 0.9155:  20%|██████████████████████████████████                                                                                                                                            | 98/500 [04:54<20:08,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 87 | Best val score -0.918312 | DKL-prior 0.000490 | BCE 0.616983 | auROC 0.9183:  27%|███████████████████████████████████████████████                                                                                                                              | 136/500 [06:58<18:39,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 75 | Best val score -0.913309 | DKL-prior 0.000497 | BCE 0.581592 | auROC 0.9133:  25%|██████████████████████████████████████████▉                                                                                                                                  | 124/500 [06:21<19:15,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 91 | Best val score -0.919976 | DKL-prior 0.000475 | BCE 0.610277 | auROC 0.9200:  28%|████████████████████████████████████████████████▍                                                                                                                            | 140/500 [07:11<18:29,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 91\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'alpha-beta.csv'))\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None).scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)\n",
    "    run_name = f\"mvib.bimodal.{joint_posterior}.alpha+beta-only.rep-{i}\"\n",
    "    trainer.save_checkpoint(checkpoint, folder='./', filename=os.path.join(RESULTS_BASE, f\"{run_name}.pth\"))\n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=None)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6c4fc",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3b+CDR3a (AVIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc9eeb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 78 | Best val score -0.935460 | DKL-prior 0.000474 | BCE 0.635196 | auROC 0.9355:  25%|███████████████████████████████████████████▍                                                                                                                               | 127/500 [20:39<1:00:41,  9.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 52 | Best val score -0.935776 | DKL-prior 0.000561 | BCE 0.525582 | auROC 0.9358:  20%|██████████████████████████████████▌                                                                                                                                        | 101/500 [15:57<1:03:02,  9.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 66 | Best val score -0.940779 | DKL-prior 0.000573 | BCE 0.502048 | auROC 0.9408:  23%|███████████████████████████████████████▎                                                                                                                                   | 115/500 [17:57<1:00:08,  9.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 78 | Best val score -0.940502 | DKL-prior 0.000384 | BCE 0.578420 | auROC 0.9405:  25%|███████████████████████████████████████████▉                                                                                                                                 | 127/500 [19:27<57:07,  9.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 73 | Best val score -0.943219 | DKL-prior 0.000422 | BCE 0.516536 | auROC 0.9432:  24%|████████████████████████████████████████████▍                                                                                                                                         | 122/500 [18:42<57:59,  9.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 73\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'alpha-beta.csv'))\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col='tcra').scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"trimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    run_name = f\"mvib.trimodal.{joint_posterior}.alpha+beta-only.rep-{i}\"\n",
    "    trainer.save_checkpoint(checkpoint, folder='./', filename=os.path.join(RESULTS_BASE, f\"{run_name}.pth\"))\n",
    "\n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=ds_test.cdr3a)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd7a799",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3a (AVIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b299ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 84 | Best val score -0.924685 | DKL-prior 0.000489 | BCE 0.561063 | auROC 0.9247:  27%|████████████████████████████████████████████████▍                                                                                                                                     | 133/500 [06:11<17:04,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 52 | Best val score -0.919000 | DKL-prior 0.000524 | BCE 0.511463 | auROC 0.9190:  20%|████████████████████████████████████▊                                                                                                                                                 | 101/500 [04:41<18:33,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 66 | Best val score -0.930090 | DKL-prior 0.000519 | BCE 0.452993 | auROC 0.9301:  23%|█████████████████████████████████████████▊                                                                                                                                            | 115/500 [05:20<17:53,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 160 | Best val score -0.924717 | DKL-prior 0.000513 | BCE 0.894604 | auROC 0.9247:  42%|███████████████████████████████████████████████████████████████████████████▋                                                                                                         | 209/500 [09:40<13:28,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 84 | Best val score -0.929822 | DKL-prior 0.000501 | BCE 0.516446 | auROC 0.9298:  27%|████████████████████████████████████████████████▍                                                                                                                                     | 133/500 [06:11<17:05,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 84\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'alpha-beta.csv'))\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcra').scaler\n",
    "    # we pass column `tcra` to `cdr3b_col` because TCRDataset expects to have the CDR3b attribute\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcra', scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    run_name = f\"mvib.bimodal-alpha.{joint_posterior}.alpha+beta-only.rep-{i}\"\n",
    "    trainer.save_checkpoint(checkpoint, folder='./', filename=os.path.join(RESULTS_BASE, f\"{run_name}.pth\"))\n",
    "\n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f91524",
   "metadata": {},
   "source": [
    "# beta set (AVIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b1b0962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 174 | Best val score -0.832874 | DKL-prior 0.000496 | BCE 0.800350 | auROC 0.8329:  45%|████████████████████████████████████████████████████████████████████████████████▋                                                                                                    | 223/500 [15:37<19:24,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 288 | Best val score -0.837785 | DKL-prior 0.000621 | BCE 0.872121 | auROC 0.8378:  67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                           | 337/500 [23:14<11:14,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 170 | Best val score -0.834989 | DKL-prior 0.000544 | BCE 0.899107 | auROC 0.8350:  44%|███████████████████████████████████████████████████████████████████████████████▎                                                                                                     | 219/500 [15:14<19:33,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 274 | Best val score -0.846585 | DKL-prior 0.000608 | BCE 0.862442 | auROC 0.8466:  65%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                | 323/500 [22:10<12:08,  4.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 172 | Best val score -0.833392 | DKL-prior 0.000653 | BCE 0.797987 | auROC 0.8334:  44%|████████████████████████████████████████████████████████████████████████████████                                                                                                     | 221/500 [15:28<19:31,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 172\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'beta.csv'))\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None).scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    run_name = f\"mvib.bimodal.{joint_posterior}.beta-only.rep-{i}\"\n",
    "    trainer.save_checkpoint(checkpoint, folder='./', filename=os.path.join(RESULTS_BASE, f\"{run_name}.pth\"))\n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=None)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b51a2d4",
   "metadata": {},
   "source": [
    "# full set: alpha+beta set + beta set (AVIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c62d12c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 180 | Best val score -0.815843 | DKL-prior 0.000677 | BCE 0.755239 | auROC 0.8158:  46%|██████████████████████████████████████████████████████████████████████████████████▉                                                                                                  | 229/500 [27:47<32:53,  7.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 150 | Best val score -0.806158 | DKL-prior 0.000565 | BCE 0.758233 | auROC 0.8062:  40%|████████████████████████████████████████████████████████████████████████                                                                                                             | 199/500 [24:05<36:25,  7.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 215 | Best val score -0.813805 | DKL-prior 0.000642 | BCE 0.753081 | auROC 0.8138:  53%|███████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 264/500 [31:53<28:30,  7.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 162 | Best val score -0.814897 | DKL-prior 0.000554 | BCE 0.699809 | auROC 0.8149:  42%|████████████████████████████████████████████████████████████████████████████▍                                                                                                        | 211/500 [25:29<34:54,  7.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 272 | Best val score -0.816647 | DKL-prior 0.000692 | BCE 0.791097 | auROC 0.8166:  64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                | 321/500 [38:56<21:42,  7.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 272\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'beta.csv'))\n",
    "df2 = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'alpha-beta.csv'))\n",
    "df = pd.concat([df1, df2]).reset_index()\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None).scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    run_name = f\"mvib.bimodal.{joint_posterior}.full.rep-{i}\"\n",
    "    trainer.save_checkpoint(checkpoint, folder='./', filename=os.path.join(RESULTS_BASE, f\"{run_name}.pth\"))\n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=None)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8ddc4-830d-4c1e-a703-ef8887a512d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# MVIB multimodal pooling of experts (poe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dba8cc00-9619-4143-8912-c5c1d1279918",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_posterior = \"poe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b951b-c003-4486-97ef-d6895dc6d0e6",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3b (MVIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34925e2c-5802-4ef3-85ae-2d8b33f352ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 115 | Best val score -0.913730 | DKL-prior 0.000494 | BCE 0.811390 | auROC 0.9137:  33%|████████████████████████████████████████████████████████▍                                                                                                                   | 164/500 [07:37<15:38,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 49 | Best val score -0.915488 | DKL-prior 0.000439 | BCE 0.505180 | auROC 0.9155:  20%|██████████████████████████████████                                                                                                                                            | 98/500 [04:54<20:08,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 87 | Best val score -0.918312 | DKL-prior 0.000490 | BCE 0.616983 | auROC 0.9183:  27%|███████████████████████████████████████████████                                                                                                                              | 136/500 [06:58<18:39,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 75 | Best val score -0.913309 | DKL-prior 0.000497 | BCE 0.581592 | auROC 0.9133:  25%|██████████████████████████████████████████▉                                                                                                                                  | 124/500 [06:21<19:15,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 91 | Best val score -0.919976 | DKL-prior 0.000475 | BCE 0.610277 | auROC 0.9200:  28%|████████████████████████████████████████████████▍                                                                                                                            | 140/500 [07:11<18:29,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 91\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'alpha-beta.csv'))\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None).scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)\n",
    "    run_name = f\"mvib.bimodal.{joint_posterior}.alpha+beta-only.rep-{i}\"\n",
    "    trainer.save_checkpoint(checkpoint, folder='./', filename=os.path.join(RESULTS_BASE, f\"{run_name}.pth\"))\n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=None)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770deae1-f4d0-46cd-b6da-19770ced4d3a",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3b+CDR3a (MVIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362ad44e-3189-494f-89a0-36ccebd8ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 78 | Best val score -0.935460 | DKL-prior 0.000474 | BCE 0.635196 | auROC 0.9355:  25%|███████████████████████████████████████████▍                                                                                                                               | 127/500 [20:39<1:00:41,  9.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 52 | Best val score -0.935776 | DKL-prior 0.000561 | BCE 0.525582 | auROC 0.9358:  20%|██████████████████████████████████▌                                                                                                                                        | 101/500 [15:57<1:03:02,  9.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 66 | Best val score -0.940779 | DKL-prior 0.000573 | BCE 0.502048 | auROC 0.9408:  23%|███████████████████████████████████████▎                                                                                                                                   | 115/500 [17:57<1:00:08,  9.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 78 | Best val score -0.940502 | DKL-prior 0.000384 | BCE 0.578420 | auROC 0.9405:  25%|███████████████████████████████████████████▉                                                                                                                                 | 127/500 [19:27<57:07,  9.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 73 | Best val score -0.943219 | DKL-prior 0.000422 | BCE 0.516536 | auROC 0.9432:  24%|████████████████████████████████████████████▍                                                                                                                                         | 122/500 [18:42<57:59,  9.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 73\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'alpha-beta.csv'))\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col='tcra').scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"trimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    run_name = f\"mvib.trimodal.{joint_posterior}.alpha+beta-only.rep-{i}\"\n",
    "    trainer.save_checkpoint(checkpoint, folder='./', filename=os.path.join(RESULTS_BASE, f\"{run_name}.pth\"))\n",
    "\n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=ds_test.cdr3a)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c9031-5090-490a-9f28-5b1ee3205db7",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3a (MVIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77f26a63-a5bf-47e5-a47f-39515aec22ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 84 | Best val score -0.924685 | DKL-prior 0.000489 | BCE 0.561063 | auROC 0.9247:  27%|████████████████████████████████████████████████▍                                                                                                                                     | 133/500 [06:11<17:04,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 52 | Best val score -0.919000 | DKL-prior 0.000524 | BCE 0.511463 | auROC 0.9190:  20%|████████████████████████████████████▊                                                                                                                                                 | 101/500 [04:41<18:33,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 66 | Best val score -0.930090 | DKL-prior 0.000519 | BCE 0.452993 | auROC 0.9301:  23%|█████████████████████████████████████████▊                                                                                                                                            | 115/500 [05:20<17:53,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 160 | Best val score -0.924717 | DKL-prior 0.000513 | BCE 0.894604 | auROC 0.9247:  42%|███████████████████████████████████████████████████████████████████████████▋                                                                                                         | 209/500 [09:40<13:28,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 84 | Best val score -0.929822 | DKL-prior 0.000501 | BCE 0.516446 | auROC 0.9298:  27%|████████████████████████████████████████████████▍                                                                                                                                     | 133/500 [06:11<17:05,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 84\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'alpha-beta.csv'))\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcra').scaler\n",
    "    # we pass column `tcra` to `cdr3b_col` because TCRDataset expects to have the CDR3b attribute\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcra', scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    run_name = f\"mvib.bimodal-alpha.{joint_posterior}.alpha+beta-only.rep-{i}\"\n",
    "    trainer.save_checkpoint(checkpoint, folder='./', filename=os.path.join(RESULTS_BASE, f\"{run_name}.pth\"))\n",
    "\n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54760f-4922-44a6-ad07-9b407128fa7f",
   "metadata": {},
   "source": [
    "# beta set (MVIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9347894c-ab49-445b-8b6f-e6ac58b341b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 174 | Best val score -0.832874 | DKL-prior 0.000496 | BCE 0.800350 | auROC 0.8329:  45%|████████████████████████████████████████████████████████████████████████████████▋                                                                                                    | 223/500 [15:37<19:24,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 288 | Best val score -0.837785 | DKL-prior 0.000621 | BCE 0.872121 | auROC 0.8378:  67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                           | 337/500 [23:14<11:14,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 170 | Best val score -0.834989 | DKL-prior 0.000544 | BCE 0.899107 | auROC 0.8350:  44%|███████████████████████████████████████████████████████████████████████████████▎                                                                                                     | 219/500 [15:14<19:33,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 274 | Best val score -0.846585 | DKL-prior 0.000608 | BCE 0.862442 | auROC 0.8466:  65%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                | 323/500 [22:10<12:08,  4.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 172 | Best val score -0.833392 | DKL-prior 0.000653 | BCE 0.797987 | auROC 0.8334:  44%|████████████████████████████████████████████████████████████████████████████████                                                                                                     | 221/500 [15:28<19:31,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 172\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'beta.csv'))\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None).scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    run_name = f\"mvib.bimodal.{joint_posterior}.beta-only.rep-{i}\"\n",
    "    trainer.save_checkpoint(checkpoint, folder='./', filename=os.path.join(RESULTS_BASE, f\"{run_name}.pth\"))\n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=None)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641adbb-f2d6-4c98-9d30-f6f7a56bd779",
   "metadata": {},
   "source": [
    "# full set: alpha+beta set + beta set (MVIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6a50434-8505-4285-b8ac-8e86bb0897b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 180 | Best val score -0.815843 | DKL-prior 0.000677 | BCE 0.755239 | auROC 0.8158:  46%|██████████████████████████████████████████████████████████████████████████████████▉                                                                                                  | 229/500 [27:47<32:53,  7.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 150 | Best val score -0.806158 | DKL-prior 0.000565 | BCE 0.758233 | auROC 0.8062:  40%|████████████████████████████████████████████████████████████████████████                                                                                                             | 199/500 [24:05<36:25,  7.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 215 | Best val score -0.813805 | DKL-prior 0.000642 | BCE 0.753081 | auROC 0.8138:  53%|███████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 264/500 [31:53<28:30,  7.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 162 | Best val score -0.814897 | DKL-prior 0.000554 | BCE 0.699809 | auROC 0.8149:  42%|████████████████████████████████████████████████████████████████████████████▍                                                                                                        | 211/500 [25:29<34:54,  7.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 272 | Best val score -0.816647 | DKL-prior 0.000692 | BCE 0.791097 | auROC 0.8166:  64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                | 321/500 [38:56<21:42,  7.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 272\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'beta.csv'))\n",
    "df2 = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'alpha-beta.csv'))\n",
    "df = pd.concat([df1, df2]).reset_index()\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None).scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    run_name = f\"mvib.bimodal.{joint_posterior}.full.rep-{i}\"\n",
    "    trainer.save_checkpoint(checkpoint, folder='./', filename=os.path.join(RESULTS_BASE, f\"{run_name}.pth\"))\n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=None)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b505744-194b-49b3-a473-e4025b0f82cf",
   "metadata": {},
   "source": [
    "# Max pooling of experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d60f0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_posterior = \"max_pool\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda0ad1a",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3b (max pooling of experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b168b6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 63 | Best val score -0.905690 | DKL-prior 0.000380 | BCE 0.420178 | auROC 0.9057:  22%|████████████████████████████████████████▊                                                                                                                                             | 112/500 [04:48<16:39,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 165 | Best val score -0.911779 | DKL-prior 0.000534 | BCE 0.535706 | auROC 0.9118:  43%|█████████████████████████████████████████████████████████████████████████████▍                                                                                                       | 214/500 [09:10<12:15,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 73 | Best val score -0.911577 | DKL-prior 0.000416 | BCE 0.448623 | auROC 0.9116:  24%|████████████████████████████████████████████▍                                                                                                                                         | 122/500 [05:15<16:18,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 125 | Best val score -0.909825 | DKL-prior 0.000397 | BCE 0.544235 | auROC 0.9098:  35%|██████████████████████████████████████████████████████████████▉                                                                                                                      | 174/500 [07:26<13:57,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 137 | Best val score -0.915058 | DKL-prior 0.000491 | BCE 0.476750 | auROC 0.9151:  37%|███████████████████████████████████████████████████████████████████▎                                                                                                                 | 186/500 [07:59<13:29,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 137\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'alpha-beta.csv'))\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None).scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    run_name = f\"mvib.bimodal.{joint_posterior}.alpha+beta-only.rep-{i}\"\n",
    "    trainer.save_checkpoint(checkpoint, folder='./', filename=os.path.join(RESULTS_BASE, f\"{run_name}.pth\"))\n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=None)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df89b7",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3b+CDR3a  (max pooling of experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b759731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 80 | Best val score -0.926624 | DKL-prior 0.000519 | BCE 0.410618 | auROC 0.9266:  26%|██████████████████████████████████████████████▉                                                                                                                                       | 129/500 [18:08<52:10,  8.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 82 | Best val score -0.926839 | DKL-prior 0.000544 | BCE 0.441198 | auROC 0.9268:  26%|███████████████████████████████████████████████▋                                                                                                                                      | 131/500 [18:26<51:55,  8.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 94 | Best val score -0.930088 | DKL-prior 0.000574 | BCE 0.436505 | auROC 0.9301:  29%|████████████████████████████████████████████████████                                                                                                                                  | 143/500 [20:09<50:20,  8.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 91 | Best val score -0.933600 | DKL-prior 0.000523 | BCE 0.426499 | auROC 0.9336:  28%|██████████████████████████████████████████████████▉                                                                                                                                   | 140/500 [19:49<50:57,  8.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 137 | Best val score -0.937910 | DKL-prior 0.000714 | BCE 0.433803 | auROC 0.9379:  37%|███████████████████████████████████████████████████████████████████▎                                                                                                                 | 186/500 [26:13<44:16,  8.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: epoch 137\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'alpha-beta.csv'))\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col='tcra').scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"trimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    run_name = f\"mvib.trimodal.{joint_posterior}.alpha+beta-only.rep-{i}\"\n",
    "    trainer.save_checkpoint(checkpoint, folder='./', filename=os.path.join(RESULTS_BASE, f\"{run_name}.pth\"))\n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=ds_test.cdr3a)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce83121",
   "metadata": {},
   "source": [
    "# Average pooling of experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c79e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_posterior = \"avg_pool\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16c885",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3b (average pooling of experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3315a8a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: expecting '}' (3549742955.py, line 59)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 59\u001b[0;36m\u001b[0m\n\u001b[0;31m    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name.csv\"), index=False)\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: expecting '}'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'alpha-beta.csv'))\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None).scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    run_name = f\"mvib.bimodal.{joint_posterior}.alpha+beta-only.rep-{i}\"\n",
    "    trainer.save_checkpoint(checkpoint, folder='./', filename=os.path.join(RESULTS_BASE, f\"{run_name}.pth\"))\n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=None)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250724ec",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3b+CDR3a  (average pooling of experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_BASE, 'alpha-beta-splits', 'alpha-beta.csv'))\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col='tcra').scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"trimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    run_name = f\"mvib.trimodal.{joint_posterior}.alpha+beta-only.rep-{i}\"\n",
    "    trainer.save_checkpoint(checkpoint, folder='./', filename=os.path.join(RESULTS_BASE, f\"{run_name}.pth\"))\n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=ds_test.cdr3a)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(os.path.join(RESULTS_BASE, f\"{run_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f33353b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vibtcr] *",
   "language": "python",
   "name": "conda-env-vibtcr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
