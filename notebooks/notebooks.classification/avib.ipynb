{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808e2dcc",
   "metadata": {},
   "source": [
    "# Attentive Variational Information Bottleneck\n",
    "\n",
    "In this notebook, we train and test the Attentive Variational Information Bottleneck (MVIB [1] with Attention of Experts) and MVIB on all datasets.\n",
    "\n",
    "[1] Microbiome-based disease prediction with multimodal variational information bottlenecks, Grazioli et al., https://www.biorxiv.org/node/2109522.external-links.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fdd9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from vibtcr.dataset import TCRDataset\n",
    "from vibtcr.mvib.mvib import MVIB\n",
    "from vibtcr.mvib.mvib_trainer import TrainerMVIB\n",
    "\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa957077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "metrics = [\n",
    "    'auROC',\n",
    "    'Accuracy',\n",
    "    'Recall',\n",
    "    'Precision',\n",
    "    'F1 score',\n",
    "    'auPRC'\n",
    "]\n",
    "\n",
    "def pr_auc(y_true, y_prob):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    return pr_auc\n",
    "\n",
    "def get_scores(y_true, y_prob, y_pred):\n",
    "    \"\"\"\n",
    "    Compute a df with all classification metrics and respective scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = [\n",
    "        roc_auc_score(y_true, y_prob),\n",
    "        accuracy_score(y_true, y_pred),\n",
    "        recall_score(y_true, y_pred),\n",
    "        precision_score(y_true, y_pred),\n",
    "        f1_score(y_true, y_pred),\n",
    "        pr_auc(y_true, y_prob)\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(data={'score': scores, 'metrics': metrics})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "314146e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133637c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "login = os.getlogin( )\n",
    "DATA_BASE = f\"/home/{login}/Git/tcr/data/\"\n",
    "RESULTS_BASE = f\"/home/{login}/Git/tcr/notebooks/notebooks.classification/results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a04b4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "batch_size = 4096\n",
    "epochs = 500\n",
    "lr = 1e-3\n",
    "\n",
    "z_dim = 150\n",
    "early_stopper_patience = 50\n",
    "monitor = 'auROC'\n",
    "lr_scheduler_param = 10\n",
    "joint_posterior = \"aoe\"\n",
    "\n",
    "beta = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794a7f3",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f5b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_BASE + 'alpha-beta-splits/alpha-beta.csv')\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None).scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=None)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(\n",
    "        RESULTS_BASE + f\"mvib.bimodal.{joint_posterior}.alpha+beta-only.rep-{i}.csv\",\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6c4fc",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3b+CDR3a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc9eeb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 80 | Score -0.935698 | DKL-prior 0.000373 | BCE 0.549172 | auROC 0.9357:  26%|██▌       | 129/500 [09:02<26:00,  4.21s/it]\n",
      "[VAL] Best epoch 156 | Score -0.935239 | DKL-prior 0.000369 | BCE 0.870948 | auROC 0.9352:  41%|████      | 205/500 [14:21<20:39,  4.20s/it]\n",
      "[VAL] Best epoch 104 | Score -0.938586 | DKL-prior 0.000363 | BCE 0.602351 | auROC 0.9386:  31%|███       | 153/500 [10:48<24:29,  4.24s/it]\n",
      "[VAL] Best epoch 120 | Score -0.935417 | DKL-prior 0.000336 | BCE 0.739175 | auROC 0.9354:  34%|███▍      | 169/500 [11:50<23:11,  4.20s/it]\n",
      "[VAL] Best epoch 137 | Score -0.941222 | DKL-prior 0.000359 | BCE 0.763292 | auROC 0.9412:  37%|███▋      | 186/500 [13:10<22:14,  4.25s/it]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_BASE + 'alpha-beta-splits/alpha-beta.csv')\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col='tcra').scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"trimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=ds_test.cdr3a)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(\n",
    "        RESULTS_BASE + f\"mvib.trimodal.{joint_posterior}.alpha+beta-only.rep-{i}.csv\",\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd7a799",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b299ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 78 | Best val score -0.924754 | DKL-prior 0.000436 | BCE 0.562571 | auROC 0.9248:  25%|██▌       | 127/500 [03:33<10:25,  1.68s/it]\n",
      "[VAL] Best epoch 96 | Best val score -0.923685 | DKL-prior 0.000542 | BCE 0.679592 | auROC 0.9237:  29%|██▉       | 145/500 [04:04<09:57,  1.68s/it]\n",
      "[VAL] Best epoch 84 | Best val score -0.928253 | DKL-prior 0.000504 | BCE 0.545874 | auROC 0.9283:  27%|██▋       | 133/500 [03:42<10:14,  1.67s/it]\n",
      "[VAL] Best epoch 124 | Best val score -0.925423 | DKL-prior 0.000544 | BCE 0.705228 | auROC 0.9254:  35%|███▍      | 173/500 [04:49<09:07,  1.67s/it]\n",
      "[VAL] Best epoch 63 | Best val score -0.927728 | DKL-prior 0.000494 | BCE 0.450250 | auROC 0.9277:  22%|██▏       | 112/500 [03:10<10:59,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_BASE + 'alpha-beta-splits/alpha-beta.csv')\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcra').scaler\n",
    "    # we pass column `tcra` to `cdr3b_col` because TCRDataset expects to have the CDR3b attribute\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcra', scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(\n",
    "        RESULTS_BASE + f\"mvib.bimodal-alpha.{joint_posterior}.alpha+beta-only.rep-{i}.csv\",\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f91524",
   "metadata": {},
   "source": [
    "# beta set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b1b0962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 357 | Score -0.831115 | DKL-prior 0.000416 | BCE 0.816526 | auROC 0.8311:  81%|████████  | 406/500 [15:39<03:37,  2.31s/it]\n",
      "[VAL] Best epoch 216 | Score -0.835735 | DKL-prior 0.000381 | BCE 0.755226 | auROC 0.8357:  53%|█████▎    | 265/500 [10:06<08:58,  2.29s/it]\n",
      "[VAL] Best epoch 241 | Score -0.837941 | DKL-prior 0.000371 | BCE 0.718900 | auROC 0.8379:  58%|█████▊    | 290/500 [11:16<08:09,  2.33s/it]\n",
      "[VAL] Best epoch 197 | Score -0.838721 | DKL-prior 0.000365 | BCE 0.695502 | auROC 0.8387:  49%|████▉     | 246/500 [09:24<09:42,  2.30s/it]\n",
      "[VAL] Best epoch 313 | Score -0.840593 | DKL-prior 0.000391 | BCE 0.760762 | auROC 0.8406:  72%|███████▏  | 362/500 [13:50<05:16,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_BASE + 'alpha-beta-splits/beta.csv')\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None).scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=None)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(\n",
    "        RESULTS_BASE + f\"mvib.bimodal.{joint_posterior}.beta-only.rep-{i}.csv\",\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b51a2d4",
   "metadata": {},
   "source": [
    "# full set: alpha+beta set + beta set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c62d12c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 142 | Score -0.809937 | DKL-prior 0.000366 | BCE 0.643406 | auROC 0.8099:  38%|███▊      | 191/500 [12:56<20:56,  4.07s/it]\n",
      "[VAL] Best epoch 334 | Score -0.813966 | DKL-prior 0.000441 | BCE 0.724015 | auROC 0.8140:  77%|███████▋  | 383/500 [25:35<07:49,  4.01s/it]\n",
      "[VAL] Best epoch 279 | Score -0.819329 | DKL-prior 0.000404 | BCE 0.655243 | auROC 0.8193:  66%|██████▌   | 328/500 [22:09<11:37,  4.05s/it]\n",
      "[VAL] Best epoch 264 | Score -0.817222 | DKL-prior 0.000392 | BCE 0.666876 | auROC 0.8172:  63%|██████▎   | 313/500 [21:12<12:40,  4.06s/it]\n",
      "[VAL] Best epoch 323 | Score -0.814560 | DKL-prior 0.000439 | BCE 0.730162 | auROC 0.8146:  74%|███████▍  | 372/500 [25:02<08:37,  4.04s/it]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(DATA_BASE + 'alpha-beta-splits/beta.csv')\n",
    "df2 = pd.read_csv(DATA_BASE + 'alpha-beta-splits/alpha-beta.csv')\n",
    "df = pd.concat([df1, df2]).reset_index()\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None).scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=None)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(\n",
    "        RESULTS_BASE + f\"mvib.bimodal.{joint_posterior}.full.rep-{i}.csv\",\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab03cfe",
   "metadata": {},
   "source": [
    "# Max pooling of experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d60f0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_posterior = \"max_pool\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda0ad1a",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3b (max pooling of experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b168b6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 145 | Score -0.910260 | DKL-prior 0.000527 | BCE 0.478257 | auROC 0.9103:  39%|███▉      | 194/500 [05:01<07:55,  1.56s/it]\n",
      "[VAL] Best epoch 165 | Score -0.911694 | DKL-prior 0.000634 | BCE 0.503740 | auROC 0.9117:  43%|████▎     | 214/500 [05:33<07:25,  1.56s/it]\n",
      "[VAL] Best epoch 167 | Score -0.911872 | DKL-prior 0.000543 | BCE 0.538788 | auROC 0.9119:  43%|████▎     | 216/500 [05:38<07:25,  1.57s/it]\n",
      "[VAL] Best epoch 125 | Score -0.907811 | DKL-prior 0.000411 | BCE 0.527814 | auROC 0.9078:  35%|███▍      | 174/500 [04:41<08:47,  1.62s/it]\n",
      "[VAL] Best epoch 129 | Score -0.909856 | DKL-prior 0.000478 | BCE 0.445229 | auROC 0.9099:  36%|███▌      | 178/500 [04:39<08:25,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_BASE + 'alpha-beta-splits/alpha-beta.csv')\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None).scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=None)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(\n",
    "        RESULTS_BASE + f\"mvib.bimodal.{joint_posterior}.alpha+beta-only.rep-{i}.csv\",\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df89b7",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3b+CDR3a  (max pooling of experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b759731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 63 | Score -0.924964 | DKL-prior 0.000451 | BCE 0.387998 | auROC 0.9250:  22%|██▏       | 112/500 [08:22<29:00,  4.49s/it]\n",
      "[VAL] Best epoch 101 | Score -0.930486 | DKL-prior 0.000590 | BCE 0.452560 | auROC 0.9305:  30%|███       | 150/500 [11:09<26:01,  4.46s/it]\n",
      "[VAL] Best epoch 73 | Score -0.932433 | DKL-prior 0.000551 | BCE 0.414454 | auROC 0.9324:  24%|██▍       | 122/500 [09:05<28:09,  4.47s/it]\n",
      "[VAL] Best epoch 87 | Score -0.934378 | DKL-prior 0.000548 | BCE 0.447056 | auROC 0.9344:  27%|██▋       | 136/500 [09:56<26:37,  4.39s/it]\n",
      "[VAL] Best epoch 203 | Score -0.935721 | DKL-prior 0.000796 | BCE 0.504529 | auROC 0.9357:  50%|█████     | 252/500 [18:36<18:19,  4.43s/it]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_BASE + 'alpha-beta-splits/alpha-beta.csv')\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col='tcra').scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"trimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=ds_test.cdr3a)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(\n",
    "        RESULTS_BASE + f\"mvib.trimodal.{joint_posterior}.alpha+beta-only.rep-{i}.csv\",\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce83121",
   "metadata": {},
   "source": [
    "# Average pooling fo experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c79e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_posterior = \"avg_pool\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16c885",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3b (average pooling of experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3315a8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 80 | Score -0.906532 | DKL-prior 0.000477 | BCE 0.529231 | auROC 0.9065:  26%|██▌       | 129/500 [03:11<09:11,  1.49s/it]\n",
      "[VAL] Best epoch 82 | Score -0.908556 | DKL-prior 0.000472 | BCE 0.513567 | auROC 0.9086:  26%|██▌       | 131/500 [03:16<09:13,  1.50s/it]\n",
      "[VAL] Best epoch 99 | Score -0.909954 | DKL-prior 0.000468 | BCE 0.533864 | auROC 0.9100:  30%|██▉       | 148/500 [03:42<08:48,  1.50s/it]\n",
      "[VAL] Best epoch 125 | Score -0.906920 | DKL-prior 0.000465 | BCE 0.675006 | auROC 0.9069:  35%|███▍      | 174/500 [04:22<08:10,  1.51s/it]\n",
      "[VAL] Best epoch 141 | Score -0.910486 | DKL-prior 0.000454 | BCE 0.726459 | auROC 0.9105:  38%|███▊      | 190/500 [05:08<08:23,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_BASE + 'alpha-beta-splits/alpha-beta.csv')\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None).scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col=None, scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"bimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=None)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(\n",
    "        RESULTS_BASE + f\"mvib.bimodal.{joint_posterior}.alpha+beta-only.rep-{i}.csv\",\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250724ec",
   "metadata": {},
   "source": [
    "# alpha+beta set - peptide+CDR3b+CDR3a  (average pooling of experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca5c6558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL] Best epoch 148 | Score -0.927377 | DKL-prior 0.000564 | BCE 0.684856 | auROC 0.9274:  39%|███▉      | 197/500 [13:49<21:15,  4.21s/it]\n",
      "[VAL] Best epoch 115 | Score -0.925632 | DKL-prior 0.000579 | BCE 0.537084 | auROC 0.9256:  33%|███▎      | 164/500 [11:28<23:31,  4.20s/it]\n",
      "[VAL] Best epoch 195 | Score -0.933327 | DKL-prior 0.000555 | BCE 0.697255 | auROC 0.9333:  49%|████▉     | 244/500 [16:59<17:49,  4.18s/it]\n",
      "[VAL] Best epoch 120 | Score -0.927332 | DKL-prior 0.000567 | BCE 0.632198 | auROC 0.9273:  34%|███▍      | 169/500 [11:51<23:13,  4.21s/it]\n",
      "[VAL] Best epoch 128 | Score -0.936024 | DKL-prior 0.000581 | BCE 0.537664 | auROC 0.9360:  35%|███▌      | 177/500 [12:33<22:54,  4.26s/it]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_BASE + 'alpha-beta-splits/alpha-beta.csv')\n",
    "\n",
    "for i in range(5):  # 5 independent train/test splits\n",
    "    set_random_seed(i)\n",
    "\n",
    "    df_train, df_test = train_test_split(df.copy(), test_size=0.2, random_state=i)\n",
    "    scaler = TCRDataset(df_train.copy(), torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col='tcra').scaler\n",
    "\n",
    "    ds_test = TCRDataset(df_test, torch.device(\"cpu\"), cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.sign, random_state=i)\n",
    "        \n",
    "    # train loader with balanced sampling\n",
    "    ds_train = TCRDataset(df_train, device, cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_train[df_train.sign == 0].shape[0], df_train[df_train.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_train.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # val loader with balanced sampling\n",
    "    ds_val = TCRDataset(df_val, device, cdr3b_col='tcrb', cdr3a_col='tcra', scaler=scaler)\n",
    "    class_count = np.array([df_val[df_val.sign == 0].shape[0], df_val[df_val.sign == 1].shape[0]])\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = torch.tensor([weight[s] for s in df_val.sign])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    model = MVIB(z_dim=z_dim, device=device, joint_posterior=joint_posterior).to(device)\n",
    "\n",
    "    trainer = TrainerMVIB(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        beta=beta,\n",
    "        checkpoint_dir=\".\",\n",
    "        mode=\"trimodal\",\n",
    "        lr_scheduler_param=lr_scheduler_param\n",
    "    )\n",
    "    checkpoint = trainer.train(train_loader, val_loader, early_stopper_patience, monitor)    \n",
    "    \n",
    "    # test\n",
    "    model = MVIB.from_checkpoint(checkpoint, torch.device(\"cpu\"))\n",
    "    pred = model.classify(pep=ds_test.pep, cdr3b=ds_test.cdr3b, cdr3a=ds_test.cdr3a)\n",
    "    pred = pred.detach().numpy()\n",
    "    df_test['prediction_'+str(i)] = pred.squeeze().tolist()\n",
    "\n",
    "    # save results for further analysis\n",
    "    df_test.to_csv(\n",
    "        RESULTS_BASE + f\"mvib.trimodal.{joint_posterior}.alpha+beta-only.rep-{i}.csv\",\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f33353b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
